# SGD
Implementation of Stochastic Gradient Descent algorithms in Python

This is the class file that implements:  
  (i) Stochastic Gradient Descent,   
  (ii) SGD with Momentum,  
  (iii) NAG,  
  (iv) AdaGrad,  
  (iv) RMSprop,  
  (vi) Adam,  
  (vii) Adamax,  
  (viii) Adadelta, 
  (ix) Nadam, 
  (x) SAG,  
  (xi) minibatch SGD,  
  (xii) SVRG.  

*NOTE*: Currently, the stopping conditions are maximum number of iteration and 2nd norm of gradient vector.
Time-delay and exponential learnong schedules are implemented.

Download this file and use *import SGD as sgd* to use the algorithms.  
See *sgd_demo.py* for an example.
